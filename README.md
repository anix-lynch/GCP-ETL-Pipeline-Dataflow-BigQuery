

## 🚀 Building an ETL Pipeline in GCP: Loading GCS Data to BigQuery via Dataflow

In this project, we’ll create an **ETL pipeline** using **Google Cloud Platform (GCP)** services. The goal is to extract raw data from **Google Cloud Storage (GCS)**, clean and transform it using **Apache Beam** and **Dataflow**, and load it into **BigQuery** for analysis. Ready to see how data moves through the cloud? Let’s dive in! 🌩️💻

## Project Overview

This project focuses on building an end-to-end **ETL pipeline** using **GCP**. We’ll be working with **Airbnb data** stored in a cloud storage bucket, processing it using **Apache Beam’s Python SDK** for cleaning and transformation, and finally loading the processed data into **BigQuery** tables for analysis.

The ETL pipeline will ensure data validation, integrity checks, cleaning, and enrichment before moving it to **BigQuery**. We’ll also set up proper access control using **GCP Identity and Access Management (IAM)** and monitor the entire process through **Dataflow** monitoring tools. By the end, you’ll have a functional pipeline that runs smoothly in a cloud environment.

### Key Features:

- 🏗️ **ETL Pipeline**: Build an ETL pipeline using **Apache Beam** to extract, transform, and load data.
- 🧹 **Data Cleaning**: Perform data validation, clean unnecessary columns, and handle missing values.
- 📊 **Data Transformation**: Enrich data through filtering, grouping, and aggregation techniques.
- 🌐 **Cloud Integration**: Set up a **Google Cloud Storage bucket**, **BigQuery dataset**, tables, and views.
- 🔒 **Access Control**: Implement access control with **GCP IAM** using service accounts for secure pipeline execution.
- 📈 **Monitoring**: Monitor the ETL pipeline execution using **Dataflow**’s built-in monitoring tools.

## 🛠 Technologies Used

- **GCP**: Core platform for storage, data processing, and analysis.
- **Apache Beam**: For data processing and pipeline creation.
- **BigQuery**: To store and analyze the cleaned and transformed data.
- **Python**: For developing the ETL pipeline using Apache Beam’s SDK.

## 🤖 Skills Applied

- Data Pipeline Engineering
- Data Extraction, Cleaning, and Transformation
- Google Cloud Platform (GCP) Services
- BigQuery and Cloud Dataflow
- Apache Beam

## Example Tasks You Can Do

- **Ingest Data from GCS**: Extract raw data from a Google Cloud Storage bucket for processing.
- **Data Validation & Cleaning**: Clean the data by removing special characters, null values, and unnecessary columns.
- **Data Transformation**: Apply aggregation, filtering, and enrichment techniques to transform the data.
- **Load to BigQuery**: Store the processed data into BigQuery tables and views for analysis.

