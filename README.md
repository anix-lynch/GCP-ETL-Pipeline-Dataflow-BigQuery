

## ğŸš€ Building an ETL Pipeline in GCP: Loading GCS Data to BigQuery via Dataflow

In this project, weâ€™ll create an **ETL pipeline** using **Google Cloud Platform (GCP)** services. The goal is to extract raw data from **Google Cloud Storage (GCS)**, clean and transform it using **Apache Beam** and **Dataflow**, and load it into **BigQuery** for analysis. Ready to see how data moves through the cloud? Letâ€™s dive in! ğŸŒ©ï¸ğŸ’»

## Project Overview

This project focuses on building an end-to-end **ETL pipeline** using **GCP**. Weâ€™ll be working with **Airbnb data** stored in a cloud storage bucket, processing it using **Apache Beamâ€™s Python SDK** for cleaning and transformation, and finally loading the processed data into **BigQuery** tables for analysis.

The ETL pipeline will ensure data validation, integrity checks, cleaning, and enrichment before moving it to **BigQuery**. Weâ€™ll also set up proper access control using **GCP Identity and Access Management (IAM)** and monitor the entire process through **Dataflow** monitoring tools. By the end, youâ€™ll have a functional pipeline that runs smoothly in a cloud environment.

### Key Features:

- ğŸ—ï¸ **ETL Pipeline**: Build an ETL pipeline using **Apache Beam** to extract, transform, and load data.
- ğŸ§¹ **Data Cleaning**: Perform data validation, clean unnecessary columns, and handle missing values.
- ğŸ“Š **Data Transformation**: Enrich data through filtering, grouping, and aggregation techniques.
- ğŸŒ **Cloud Integration**: Set up a **Google Cloud Storage bucket**, **BigQuery dataset**, tables, and views.
- ğŸ”’ **Access Control**: Implement access control with **GCP IAM** using service accounts for secure pipeline execution.
- ğŸ“ˆ **Monitoring**: Monitor the ETL pipeline execution using **Dataflow**â€™s built-in monitoring tools.

## ğŸ›  Technologies Used

- **GCP**: Core platform for storage, data processing, and analysis.
- **Apache Beam**: For data processing and pipeline creation.
- **BigQuery**: To store and analyze the cleaned and transformed data.
- **Python**: For developing the ETL pipeline using Apache Beamâ€™s SDK.

## ğŸ¤– Skills Applied

- Data Pipeline Engineering
- Data Extraction, Cleaning, and Transformation
- Google Cloud Platform (GCP) Services
- BigQuery and Cloud Dataflow
- Apache Beam

## Example Tasks You Can Do

- **Ingest Data from GCS**: Extract raw data from a Google Cloud Storage bucket for processing.
- **Data Validation & Cleaning**: Clean the data by removing special characters, null values, and unnecessary columns.
- **Data Transformation**: Apply aggregation, filtering, and enrichment techniques to transform the data.
- **Load to BigQuery**: Store the processed data into BigQuery tables and views for analysis.

